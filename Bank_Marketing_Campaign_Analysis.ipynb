{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bank Marketing Campaign Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGxPw4IqNIto",
        "colab_type": "text"
      },
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "> **Business Use Case**\n",
        "\n",
        "*  There has been a revenue decline for a Portuguese bank and they would like to know what actions to take. After investigation, they found out that the root cause is that their clients are not depositing as frequently as before. Knowing that term deposits allow banks to hold onto a deposit for a specific amount of time, so banks can invest in higher gain financial products to make a profit. In addition, banks also hold better chance to persuade term deposit clients into buying other products such as funds or insurance to further increase their revenues. As a result, the Portuguese bank would like to identify existing clients that have higher chance to subscribe for a term deposit and focus marketing efforts on such clients.\n",
        "\n",
        "> **Data Science Problem Statement**\n",
        "\n",
        "*   Predict if the client will subscribe to a term deposit based on the analysis of the marketing campaigns the bank performed.\n",
        "\n",
        "\n",
        ">**Evaluation Metric**\n",
        "\n",
        "*   We will be using [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) for evaluation.\n",
        "\n",
        ">**Word of caution**\n",
        "\n",
        "* This template is just an example of a data-science pipeline, every data science problem is unique and there are multiple ways to tackle them. Go through this template and try to leverage the information in this while solving your hackathon problems but you may not be able to use all the functions created here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVIOjsRvVfyX",
        "colab_type": "text"
      },
      "source": [
        "#**Understanding the dataset**\n",
        "\n",
        "**Data Set Information**\n",
        "\n",
        "The data is related to direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be subscribed ('yes') or not ('no') subscribed.\n",
        "\n",
        "There are two datasets:\n",
        "`train.csv` with all examples (32950) and 21 inputs including the target feature, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
        "\n",
        "`test.csv` which is the test data that consists  of 8238 observations and 20 features without the target feature\n",
        "\n",
        "Goal:- The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).\n",
        "\n",
        "**Features**\n",
        "\n",
        "|Feature|Feature_Type|Description|\n",
        "|-----|-----|-----|\n",
        "|age|numeric|age of a person|  \n",
        "|job |Categorigol,nominal|type of job ('admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')|  \n",
        "|marital|categorical,nominal|marital status ('divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)|  \n",
        "|education|categorical,nominal| ('basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown') | \n",
        "|default|categorical,nominal| has credit in default? ('no','yes','unknown')|  \n",
        "|housing|categorical,nominal| has housing loan? ('no','yes','unknown')|  \n",
        "|loan|categorical,nominal| has personal loan? ('no','yes','unknown')|  \n",
        "|contact|categorical,nominal| contact communication type ('cellular','telephone')|  \n",
        "|month|categorical,ordinal| last contact month of year ('jan', 'feb', 'mar', ..., 'nov', 'dec')| \n",
        "|day_of_week|categorical,ordinal| last contact day of the week ('mon','tue','wed','thu','fri')|  \n",
        "|duration|numeric| last contact duration, in seconds . Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no')|\n",
        "|campaign|numeric|number of contacts performed during this campaign and for this client (includes last contact)|  \n",
        "|pdays|numeric| number of days that passed by after the client was last contacted from a previous campaign (999 means client was not previously contacted)|  \n",
        "|previous|numeric| number of contacts performed before this campaign and for this client|  \n",
        "|poutcome|categorical,nominal| outcome of the previous marketing campaign ('failure','nonexistent','success')|  \n",
        "|emp.var.rate|numeric|employment variation rate - quarterly indicator|  \n",
        "|cons.price.idx|numeric| consumer price index - monthly indicator|  \n",
        "|cons.conf.idx|numeric| consumer confidence index - monthly indicator|  \n",
        "|euribor3m|numeric|euribor 3 month rate - daily indicator|\n",
        "|nr.employed|numeric| number of employees - quarterly indicator|   \n",
        "\n",
        "**Target variable (desired output):**  \n",
        "\n",
        "|Feature|Feature_Type|Description|\n",
        "|-----|-----|-----|\n",
        "|y | binary| has the client subscribed a term deposit? ('yes','no')|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9367ldCoV9FP",
        "colab_type": "text"
      },
      "source": [
        "###  Importing necessary libraries\n",
        "\n",
        "The following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0SdRYBmWFT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "87e5e7a1-e257-4c96-e097-5bff41fc4a5f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFGQBgr4WSeM",
        "colab_type": "text"
      },
      "source": [
        "### Loading Data Modelling Libraries\n",
        "\n",
        "We will use the popular scikit-learn library to develop our machine learning algorithms. In sklearn, algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the matplotlib and seaborn library. Below are common classes to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHmqldiwWoeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler,StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier ,RandomForestClassifier ,GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import Ridge,Lasso\n",
        "from sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,roc_curve,confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from xgboost import XGBClassifier \n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_columns',None)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "braduYOmW16g",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading and Cleaning\n",
        "### Load and Prepare dataset\n",
        "\n",
        "- In this task, we'll load the dataframe in pandas, drop the unnecessary columns and display the top five rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFsoL4ROW-ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "path = 'data/train.csv'\n",
        "# Load the dataframe\n",
        "dataframe = pd.read_csv(path,delimiter=',')\n",
        "\n",
        "# Remove the Id column from the dataset\n",
        "\n",
        "dataframe.drop('Id',axis=1,inplace=True)\n",
        "dataframe.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TE739eY1KV",
        "colab_type": "text"
      },
      "source": [
        "### Check Numeric and Categorical Features\n",
        "\n",
        "If you are familiar with machine learning, you will know that a dataset consists of numerical and categorical columns.\n",
        "\n",
        "Looking at the dataset, we think we can identify the categorical and continuous columns in it. Right? But it might also be possible that the numerical values are represented as strings in some feature. Or the categorical values in some features might be represented as some other datatypes instead of strings. Hence it's good to check for the datatypes of all the features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyhz3zLSY6F0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function to identify numeric features\n",
        "def numeric_features(dataset):\n",
        "    numeric_col = dataset.select_dtypes(include=np.number).columns.tolist()\n",
        "    return dataset[numeric_col].head()\n",
        "    \n",
        "numeric_columns = numeric_features(dataframe)\n",
        "print(numeric_columns)\n",
        "\n",
        "print(\"====\"*20)\n",
        "\n",
        "\n",
        "\n",
        "# Function to identify categorical features\n",
        "def categorical_features(dataset):\n",
        "    categorical_col = dataset.select_dtypes(exclude=np.number).columns.tolist()\n",
        "    return dataset[categorical_col].head()\n",
        "\n",
        "categorical_columns = categorical_features(dataframe)\n",
        "print(\"Categorical Features:\")\n",
        "print(categorical_columns)\n",
        "\n",
        "\n",
        "# Function to check the datatypes of all the columns:\n",
        "def check_datatypes(dataset):\n",
        "    return dataset.dtypes\n",
        "\n",
        "\n",
        "check_datatypes(dataframe)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCo5nX9Y_Aw",
        "colab_type": "text"
      },
      "source": [
        "### Check Missing Data \n",
        "\n",
        "One of the main steps in data preprocessing is handling missing data. Missing data means absence of observations in columns that can be caused while procuring the data, lack of information, incomplete results etc. Feeding missing data to your machine learning model could lead to wrong prediction or classification. Hence it is necessary to identify missing values and treat them.\n",
        "\n",
        "- In the function below, we calculate the total missing values and the percentage of missing values in every feature of the dataset.\n",
        "- The function ideally returns a dataframe consisting of the feature names as index and two columns having the count and percentage of missing values in that feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uRS-DcgZGg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to identify the number of missing values in every feature\n",
        "def missing_data(dataset):\n",
        "    total = dataset.isnull().sum().sort_values(ascending=False)\n",
        "    percent = (dataset.isnull().sum()/dataset.isnull().count()).sort_values(ascending=False)\n",
        "    missing_data = pd.concat([total,percent],axis=1, keys=['Total','Percent'])\n",
        "    return missing_data\n",
        "        \n",
        "miss4ing_data = missing_data(dataframe)\n",
        "print(missing_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooSftHm_aVZH",
        "colab_type": "text"
      },
      "source": [
        "### Dropping missing values\n",
        "\n",
        "The given dataset is a pretty clean dataset. But this might not be the case always as you can often encounter missing values represented as `NaN` values  in the data. \n",
        "\n",
        "There are two methods of dealing with missing data \n",
        "- Dropping them\n",
        "- Imputing them.\n",
        "\n",
        "Depending on the case we can allow a specific proportion of missing values, beyond which we might want to drop the variable from analysis.\n",
        "\n",
        "This varies from case to case on the amount of information you think the variable has. For example, if you are working on some dataset which contains a column for date of marriage. It may be blank for 50% (or even more) of the population, but might have very high information about the lifestyle of the person. In such cases, you would still use the variable.\n",
        "\n",
        "If the information contained in the variable is not that high, you can drop the variable if it has more than 50% missing values. There are projects / models where imputation of even 20 - 30% missing values provided better results - the famous Titanic dataset on Kaggle being one such case. Age is missing in ~20% of cases, but you benefit by imputing them rather than ignoring the variable.\n",
        "\n",
        "- Now you have the number and percentage of missing values in every feature, from the previous function. \n",
        "- Using this information, you can decide as to what proportion of missing values you should remove from every feature.\n",
        "- The function below takes a threshold value of your choice and removes the features having missing value percentage greater than this threshold. The function can take three parameters - the dataframe, missing data dataframe and threshold value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWZSK9GdaZoK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to drop missing values\n",
        "def drop_missing(dataset, missing,value):\n",
        "    dataset = dataset.drop((missing[missing['Percent'] > value]).index, axis=1)\n",
        "    print(dataset.isnull().sum().sort_values(ascending=False))\n",
        "    return dataset\n",
        "    \n",
        "dataframe = drop_missing(dataframe,missing_data,0.60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140YDJWobBEV",
        "colab_type": "text"
      },
      "source": [
        "### Fill null values in continuous features\n",
        "\n",
        "There are no null values in any of the continuous columns in this dataset. But when null values exist in a continuous column, a good approach would be to impute them.\n",
        "\n",
        "There exists many approach to missing-data imputation and they usually depend on your problem and how your data algorithm behaves. If the features are numeric you can use simple approaches, such as average values and sampling from the feature distribution.\n",
        "\n",
        "- Missing values in continuous data are mostly imputed using mean or median. What to choose depends on a lot of factors and is to be decided by you\n",
        "- Let's write a function that will take the dataframe and the impute missing data with either mean or mode, depending on the user's choice.\n",
        "    - For this, we define a parameter that can take only two values 0 or 1.\n",
        "    - If you specify 0 - missing values are imputed with *mean*\n",
        "    - If you specify 1 - missing values are imputed with *median*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEmBkt__zRhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function to impute missing values with mean or median\n",
        "def fill_null_values(dataset, value):\n",
        "    nulls = pd.DataFrame(dataset.isnull().sum().sort_values(ascending=False))\n",
        "    nulls = nulls[nulls>0]\n",
        "    nulls.columns = ['Null Count']\n",
        "    nulls.index.name = 'Feature'\n",
        "    \n",
        "\n",
        "    for column in nulls[nulls[\"Null Count\"]> 0].index:\n",
        "        if dataset[column].dtype == np.number and value == 0:\n",
        "            dataset[column].fillna(dataset[column].mean(), inplace = True)\n",
        "        elif value == 1:\n",
        "            dataset[column].fillna(dataset[column].median(), inplace = True)\n",
        "        else:\n",
        "            dataset[column].fillna(\"NA\", inplace = True)\n",
        "    print(dataset.isnull().sum())\n",
        "    return dataset\n",
        "\n",
        "dataframe=fill_null_values(dataframe,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxr6srnnzW17",
        "colab_type": "text"
      },
      "source": [
        "### Check for Class Imbalance\n",
        "\n",
        "Class imbalance occurs when the observations belonging to one class in the target are significantly higher than the other class or classes. A class distribution of **80:20 or greater** is typically considered as an imbalance for a binary classification. \n",
        "\n",
        "Since most machine learning algorithms assume that data is equally distributed, applying them on imbalanced data often results in bias towards majority classes and poor classification of minority classes. Hence we need to identify & deal with class imbalance. \n",
        "\n",
        "Let's write a function below that takes the target variable and outputs the distribution of classes in the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-8QLnpzbze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_imbalance(target):\n",
        "    class_values = (target.value_counts()/target.value_counts().sum())*100\n",
        "    return class_values\n",
        "\n",
        "class_imbalance(dataframe['y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qgu3kN0zgA8",
        "colab_type": "text"
      },
      "source": [
        "### Observations : \n",
        "- The class distribution in the target is ~89:11. This is a clear indication of imbalance.\n",
        "\n",
        "### Detect outliers in the continuous columns\n",
        "\n",
        "Outliers are observations that lie far away from majority of observations in the dataset and can be represented mathematically in different ways.\n",
        "\n",
        "One method of defining outliers are: outliers are data points lying beyond **(third quartile + 1.5xIQR)** and below **(first quartile - 1.5xIQR)**. \n",
        "\n",
        "- The function below takes a dataframe and outputs the number of outliers in every numeric feature based on the above rule of *IQR* \n",
        "\n",
        "You can even modify the function below to capture the outliers as per their other definitions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSBbayHznlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Function to detect outliers in every feature\n",
        "def detect_outliers(dataframe):\n",
        "    cols = list(dataframe)\n",
        "    outliers = pd.DataFrame(columns = ['Feature', 'Number of Outliers'])\n",
        "    for column in cols:\n",
        "        if column in dataframe.select_dtypes(include=np.number).columns:\n",
        "            q1 = dataframe[column].quantile(0.25)\n",
        "            q3 = dataframe[column].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            fence_low = q1 - (1.5*iqr)\n",
        "            fence_high = q3 + (1.5*iqr)\n",
        "            outliers = outliers.append({'Feature':column, 'Number of Outliers':dataframe.loc[(dataframe[column] < fence_low) | (dataframe[column] > fence_high)].shape[0]},ignore_index=True)\n",
        "    return outliers\n",
        "\n",
        "detect_outliers(dataframe)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReLmwSgHzsGO",
        "colab_type": "text"
      },
      "source": [
        "### Observations :\n",
        "- As per the IQR methodology, there are outliers in majority of the columns.\n",
        "- In the further steps below, we will see how to deal with the outliers.\n",
        "## EDA & Data Visualizations\n",
        "\n",
        "Exploratory data analysis is an approach to analyzing data sets by summarizing their main characteristics with visualizations. The EDA process is a crucial step prior to building a model in order to unravel various insights that later become important in developing a robust algorithmic model. \n",
        "###  Univariate analysis of Categorical columns\n",
        "\n",
        "Univariate analysis means analysis of a single variable. Itâ€™s mainly describes the characteristics of the variable.\n",
        "\n",
        "If the variable is categorical we can use either a bar chart or a pie chart to find the distribution of the classes in the variable.\n",
        "\n",
        "- It can get a little tedious (and boring!) to write the same piece of code for analysing the frequency of categorical variables. So why not write a single function for it and just call it whenever required.\n",
        "- The function plots the frequency of all the values in the categorical variables. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZolSASpz3zY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to perform univariate analysis of categorical columns\n",
        "def plot_categorical_columns(dataframe):\n",
        "    categorical_columns = dataframe.select_dtypes(include=['object']).columns\n",
        "    \n",
        "    for i in range(0,len(categorical_columns),2):\n",
        "            if len(categorical_columns) > i+1:\n",
        "                \n",
        "                plt.figure(figsize=(10,4))\n",
        "                plt.subplot(121)\n",
        "                dataframe[categorical_columns[i]].value_counts(normalize=True).plot(kind='bar')\n",
        "                plt.title(categorical_columns[i])\n",
        "                plt.subplot(122)     \n",
        "                dataframe[categorical_columns[i+1]].value_counts(normalize=True).plot(kind='bar')\n",
        "                plt.title(categorical_columns[i+1])\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                dataframe[categorical_columns[i]].value_counts(normalize=True).plot(kind='bar')\n",
        "                plt.title(categorical_columns[i])\n",
        "        \n",
        "        \n",
        "plot = plot_categorical_columns(dataframe)      \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UutJ0uLgz8-6",
        "colab_type": "text"
      },
      "source": [
        "### Observations :\n",
        "\n",
        "From the above visuals, we can make the following observations: \n",
        "- The top three professions that our customers belong to are - administration, blue-collar jobs and technicians.\n",
        "- A huge number of the customers are married.\n",
        "- Majority of the customers do not have a credit in default\n",
        "- Many of our past customers have applied for a housing loan but very few have applied for personal loans.\n",
        "- Cell-phones seem to be the most favoured method of reaching out to customers.\n",
        "- Many customers have been contacted in the month of **May**.\n",
        "- The plot for the target variable shows heavy imbalance in the target variable. \n",
        "- The missing values in some columns have been represented as `unknown`. `unknown` represents missing data. In the next task, we will treat these values.  \n",
        "\n",
        "### Imputing `unknown` values of categorical columns \n",
        "\n",
        "In the previous task we have seen some categorical variables have a value called `unknown`. `unknown` values are a kind of missing data.\n",
        "Depending on the use case, we can decide how to deal with these values. One method is to directly impute them with the mode value of respective columns.\n",
        "\n",
        "- The function below imputes the value `unknown` in the categorical columns with the mode value of that column. You can modify this function to replace any unwanted value(for e.g `NaN` value) in a column with a value of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp3RMsAM0Fay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Impute mising values of categorical data with mode\n",
        "def impute_mode(dataframe):\n",
        "    cols = list(dataframe)\n",
        "    for col in cols:\n",
        "        if col in dataframe.select_dtypes(exclude=np.number).columns:\n",
        "            dataframe[col]=dataframe[col].str.replace('unknown',dataframe[col].mode()[0])\n",
        "    return dataframe\n",
        "\n",
        "dataframe = impute_mode(dataframe)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNf4VCoL0I0R",
        "colab_type": "text"
      },
      "source": [
        "### Univariate analysis of Continuous columns\n",
        "Just like for categorical columns, by performing a univariate analysis on the continuous columns, we can get a sense of the distrbution of values in every column and of the outliers in the data. Histograms are great for plotting the distribution of the data and boxplots are the best choice for visualizing outliers. \n",
        "\n",
        "- Let's construct two functions, one that plots a histogram of all the continuous features and other that plots a boxplot of the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsJMZ3CW0MYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Function to plot histograms\n",
        "def plot_continuous_columns(dataframe):\n",
        "    numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()\n",
        "    dataframe = dataframe[numeric_columns]\n",
        "    \n",
        "    for i in range(0,len(numeric_columns),2):\n",
        "        if len(numeric_columns) > i+1:\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.subplot(121)\n",
        "            sns.distplot(dataframe[numeric_columns[i]], kde=False)\n",
        "            plt.subplot(122)            \n",
        "            sns.distplot(dataframe[numeric_columns[i+1]], kde=False)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            sns.distplot(dataframe[numeric_columns[i]], kde=False)\n",
        "\n",
        "# Function to plot boxplots\n",
        "def plot_box_plots(dataframe):\n",
        "    numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()\n",
        "    dataframe = dataframe[numeric_columns]\n",
        "    \n",
        "    for i in range(0,len(numeric_columns),2):\n",
        "        if len(numeric_columns) > i+1:\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.subplot(121)\n",
        "            sns.boxplot(dataframe[numeric_columns[i]])\n",
        "            plt.subplot(122)            \n",
        "            sns.boxplot(dataframe[numeric_columns[i+1]])\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            sns.boxplot(dataframe[numeric_columns[i]])\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "plot_continuous_columns(dataframe)            \n",
        "plot_box_plots(dataframe)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTg8Do_Z0Toh",
        "colab_type": "text"
      },
      "source": [
        "### Observation :\n",
        "\n",
        "- As we can see from the histogram, the features `age`, `duration` and `campaign` are heavily skewed and this is due to the presence of outliers as seen in the boxplot for these features. We will deal with these outliers in the steps below.\n",
        "- Looking at the plot for `pdays`, we can infer that majority of the customers were being contacted for the first time because as per the feature description for `pdays` the value 999 indicates that the customer had not been contacted previously. \n",
        "- Since the features `pdays` and `previous` consist majorly only of a single value, their variance is quite less and hence we can drop them since technically will be of no help in prediction.\n",
        "\n",
        "### Dropping the columns `pdays` & `previous`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYDpHlY70dZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe.drop(['pdays','previous'],1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeNMxixX0hgl",
        "colab_type": "text"
      },
      "source": [
        "### Bivariate Analysis - Categorical Columns\n",
        "\n",
        "Bivariate analysis involves checking the relationship between two variables simultaneously. In the function below, we plot every categorical feature against the target by plotting a barchart. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrRIUKg0lOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bivariate_analysis_categorical(dataframe,target):\n",
        "    categorical_columns = dataframe.select_dtypes(exclude=np.number).columns\n",
        "    for i in range(0,len(categorical_columns),2):\n",
        "        if len(categorical_columns) > i+1:\n",
        "            plt.figure(figsize=(15,5))\n",
        "            plt.subplot(121)\n",
        "            sns.countplot(x=dataframe[categorical_columns[i]],hue=target,data=dataframe)\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.subplot(122)            \n",
        "            sns.countplot(dataframe[categorical_columns[i+1]],hue=target,data=dataframe)\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "bivariate_analysis_categorical(dataframe,dataframe['y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3_SetKv0sUh",
        "colab_type": "text"
      },
      "source": [
        "### Observations:\n",
        "\n",
        "- The common traits seen for customers who have subscribed for the term deposit are :\n",
        "    - Customers having administrative jobs form the majority amongst those who have subscirbed to the term deposit with technicians being the second majority.\n",
        "    - They are married \n",
        "    - They hold a university degree\n",
        "    - They do not hold a credit in default\n",
        "    - Housing loan doesn't seem a priority to check for since an equal number of customers who have and have not subscribed to it seem to have subscribed to the term deposit.\n",
        "    - Cell-phones should be the preferred mode of contact for contacting customers.\n",
        "\n",
        "### Treating outliers in the continuous columns\n",
        "\n",
        "- Outliers can be treated in a variety of ways. It depends on the skewness of the feature.\n",
        "- To reduce right skewness, we use roots or logarithms or reciprocals (roots are weakest). This is the most common problem in practice.\n",
        "- To reduce left skewness, we take squares or cubes or higher powers.\n",
        "- But in our data, some of the features have negative values and also the value 0. In such cases, square root transform or logarithmic transformation cannot be used since we cannot take square root of negative values and logarithm of zero is not defined.\n",
        "- Hence for this data we use a method called **Winsorization**. In this method we define a confidence interval of let's say 90% and then replace all the outliers below the 5th percentile with the value at 5th percentile and all the values above 95th percentile with the value at the 95th percentile. It is pretty useful when there are negative values and zeros in the features which cannot be treated with log transforms or square roots. Do read up on it more [here](https://www.statisticshowto.datasciencecentral.com/winsorize/)\n",
        "\n",
        "Lets' write a function below that treats all the outliers in the numeric features using winsorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjROKu4805ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to treat outliers \n",
        "def treat_outliers(dataframe):\n",
        "    cols = list(dataframe)\n",
        "    for col in cols:\n",
        "        if col in dataframe.select_dtypes(include=np.number).columns:\n",
        "            dataframe[col] = winsorize(dataframe[col], limits=[0.05, 0.1],inclusive=(True, True))\n",
        "    \n",
        "    return dataframe    \n",
        "\n",
        "\n",
        "#dataframe = treat_outliers(dataframe)\n",
        "\n",
        "# Checking for outliers after applying winsorization\n",
        "#detect_outliers(dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR2g5u9408eG",
        "colab_type": "text"
      },
      "source": [
        "### Observation :\n",
        "\n",
        "Using winsorization has resulted in removal of all the outliers from the numerical columns.  You can even use normalization or standardization for dealing with outliers. \n",
        "## Applying vanilla models on the data\n",
        "\n",
        "Since we have performed preprocessing on our data and also done with the EDA part, it is now time to apply vanilla machine learning models on the data and check their performance.\n",
        "### Function to Label Encode Categorical variables\n",
        "\n",
        "Before applying our machine learning algorithm, we need to recollect that any algorithm can only read numerical values. It is therefore essential to encode categorical features into numerical values. Encoding of categorical variables can be performed in two ways:\n",
        "- Label Encoding\n",
        "- One-Hot Encoding.\n",
        "\n",
        "For the given dataset, we are going to label encode the categorical columns. \n",
        "\n",
        "- In the function below we will perform label encoding on all the categorical features and also the target (since it is categorical) in the  dataset. You can modify the below function in order to perform One-Hot Encoding as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7UK3agI1KDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "le = LabelEncoder()\n",
        "# Function that auto encodes any dataframe column of type category or object.\n",
        "def dummyEncode(dataset):\n",
        "        \n",
        "        columnsToEncode = list(dataset.select_dtypes(include=['category','object']))\n",
        "        le = LabelEncoder()\n",
        "        for feature in columnsToEncode:\n",
        "            try:\n",
        "                dataset[feature] = le.fit_transform(dataset[feature])\n",
        "            except:\n",
        "                print('Error encoding '+feature)\n",
        "        return dataset\n",
        "\n",
        "dataframe = dummyEncode(dataframe)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worF4eBq1N7p",
        "colab_type": "text"
      },
      "source": [
        "### Fit vanilla classification models\n",
        "\n",
        "Since we have label encoded our categorical variables, our data is now ready for applying machine learning algorithms. \n",
        "\n",
        "There are many Classification algorithms are present in machine learning, which are used for different classification applications. Some of the main classification algorithms are as follows-\n",
        "- Logistic Regression\n",
        "- DecisionTree Classifier\n",
        "- RandomForest Classfier\n",
        "- XGBClassifier\n",
        "- GradientBoostingClassifier\n",
        "\n",
        "The function we have written below takes the features, target and the classification model as the input parameters and internally splits them into training data and validation data. It then fits the classification model on the train data and then makes a prediction on the validation data and outputs the `roc_auc_score` and the `roc_curve` for this prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgFJvsm51RsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def run_model(predictors,target, model):\n",
        "    '''\n",
        "    Performs model training and tests using ROC-AUC \n",
        "    returns AUC score\n",
        "    '''\n",
        "    x_train,x_val,y_train,y_val = train_test_split(predictors,target,test_size=0.2,random_state=42)\n",
        "    model.fit(x_train, y_train)\n",
        "    y_scores = model.predict(x_val)\n",
        "    auc = roc_auc_score(y_val, y_scores)\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_val,y_scores))\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_scores)\n",
        "    print('ROC_AUC_SCORE is',roc_auc_score(y_val, y_scores))\n",
        "    \n",
        "    #fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n",
        "    \n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "    return auc\n",
        "\n",
        "# Predictors\n",
        "X = dataframe.iloc[:,:-1]\n",
        "\n",
        "# Target\n",
        "y = dataframe.iloc[:,-1]\n",
        "\n",
        "# Choosing the models. If you want to specify additional models, kindly specify them as a key-value pair as shown below.\n",
        "models = {'Logistic Regression':LogisticRegression,'Decision Tree':DecisionTreeClassifier,'Random Forest': RandomForestClassifier,'XGBoost':XGBClassifier,'Gradient Boosting':GradientBoostingClassifier}\n",
        "\n",
        "for i in models.items():\n",
        "    # run model\n",
        "    model = i[1]()\n",
        "    auc = run_model(X, y, model) # train and returns AUC test score\n",
        "    print('AUC Score = %.2f' %(auc*100) +' %\\nOn Model - \\n'+str(i[0]))\n",
        "    print('===='*20)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKmeZaet1XM3",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection \n",
        "\n",
        "Now that we have applied vanilla models on our data, we now have a basic understanding of what our predictions look like. Let's now use feature selection methods for identifying the best set of features for each model.\n",
        "###  Using RFE for feature selection\n",
        "In this task let's use Recursive Feature Elimination for selecting the best features. RFE is a wrapper method that uses the model to identify the best features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D86OT_11joT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def feature_selection(predictors,target,number_of_features,model):\n",
        "\n",
        "    models = model()\n",
        "    rfe = RFE(models,number_of_features)\n",
        "    rfe = rfe.fit(X,y)\n",
        "    feature_ranking = pd.Series(rfe.ranking_, index=X.columns)\n",
        "    plt.show()\n",
        "    print('Features  to be selected for {} are:'.format(str(i[0])))\n",
        "    print(feature_ranking[feature_ranking.values==1].index.tolist())\n",
        "    print('===='*30)\n",
        "\n",
        "# Choosing the models. If you want to specify additional models, kindly specify them as a key-value pair as shown below.\n",
        "models = {'Logistic Regression':LogisticRegression,'Random Forest':RandomForestClassifier,'XGBoost':XGBClassifier}\n",
        "\n",
        "# Selecting 8 number of features\n",
        "for i in models.items():\n",
        "    feature_selection(X,y,8,i[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2auDqVO1oSv",
        "colab_type": "text"
      },
      "source": [
        "### Feature Selection using Random Forest\n",
        "\n",
        "Random Forests are often used for feature selection in a data science workflow. This is because the tree based strategies that random forests use, rank the features based on how well they improve the purity of the node. The nodes having a very low impurity get split at the start of the tree while the nodes having a very high impurity get split towards the end of the tree. Hence by pruning the tree after desired amount of splits, we can create a subset of the most important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqfSeA_c1sLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rfc_feature_selection(dataset,target):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=0.3, random_state=42, stratify=target)\n",
        "    rfc = RandomForestClassifier(random_state=42)\n",
        "    rfc.fit(X_train, y_train)\n",
        "    y_pred = rfc.predict(X_test)\n",
        "    rfc_importances = pd.Series(rfc.feature_importances_, index=dataset.columns).sort_values().tail(10)\n",
        "    rfc_importances.plot(kind='bar')\n",
        "    plt.show()\n",
        "\n",
        "rfc_feature_selection(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YskYJXJ1woF",
        "colab_type": "text"
      },
      "source": [
        "### Observations :\n",
        "\n",
        "We can test the features obtained from both the feature selection techniques by inserting these features to the model and depending on which set of features perform better, we can retain them for the model. \n",
        "\n",
        "__The Feature Selection techniques can differ from problem to problem and the techniques applied for this problem may or may not work for the other problems. In those cases, feel free to try out other methods like PCA, SelectKBest(), SelectPercentile(), tSNE etc.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_EtaQuW11l7",
        "colab_type": "text"
      },
      "source": [
        "# Grid-Search & Hyperparameter Tuning \n",
        "\n",
        "Hyperparameters are function attributes that we have to specify for an algorithm. By now, you should be knowing that grid search is done to find out the best set of hyperparameters for your model.  \n",
        "\n",
        "### Grid Search for Random Forest\n",
        "\n",
        "In the below task, we write a function that performs hyperparameter tuning for a random forest classifier. We have used the hyperparameters `max_features`, `max_depth` and `criterion` for this task. Feel free to play around with this function by introducing a few more hyperparameters and chaniging their values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D1gayR316f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def grid_search_random_forrest(dataframe,target):\n",
        "\n",
        "    x_train,x_val,y_train,y_val = train_test_split(X,y, test_size=0.3, random_state=42, stratify=y)\n",
        "    rfc = RandomForestClassifier()\n",
        "    param_grid = { \n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth' : [4,5,6,7,8],\n",
        "    'criterion' :['gini', 'entropy']\n",
        "    }\n",
        "    grid_search_model = GridSearchCV(rfc, param_grid=param_grid)\n",
        "    grid_search_model.fit(x_train, y_train)\n",
        "    print('Best Parameters are:')\n",
        "    return grid_search_model.best_params_\n",
        "\n",
        "\n",
        "grid_search_random_forrest(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvunA9Qg2DeE",
        "colab_type": "text"
      },
      "source": [
        "### Applying the best parameters obtained using Grid Search on Random Forest model\n",
        "\n",
        "In the task below, we fit a random forest model using the best parameters obtained using Grid Search. Since the target is imbalanced, we apply Synthetic Minority Oversampling (SMOTE) for undersampling and oversampling the majority and minority classes in the target respectively. \n",
        "\n",
        "__Kindly note that SMOTE should always be applied only on the training data and not on the validation and test data.__\n",
        "\n",
        "You can try experimenting with and without SMOTE and check for the difference in recall. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKI75gNZ2Hk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score,roc_curve,classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "def grid_search_random_forrest_best(dataframe,target):\n",
        "    \n",
        "    \n",
        "    x_train,x_val,y_train,y_val = train_test_split(dataframe,target, test_size=0.3, random_state=42)\n",
        "    \n",
        "    # Applying Smote on train data for dealing with class imbalance\n",
        "    smote = SMOTE(kind='regular')\n",
        "    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n",
        "    \n",
        "    \n",
        "    rfc = RandomForestClassifier(n_estimators=11, max_features='auto', max_depth=8, criterion='entropy',random_state=42)\n",
        "    rfc.fit(X_sm, y_sm)\n",
        "    y_pred = rfc.predict(x_val)\n",
        "\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_pred)\n",
        "    print('On Validation data')\n",
        "    print('ROC_AUC_SCORE is',roc_auc_score(y_val, y_pred))\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    #fpr, tpr, _ = roc_curve(y_test, predictions[:,1])\n",
        "    plt.clf()\n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "grid_search_random_forrest_best(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTx5eOcf2Px5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgtoykiY2UL1",
        "colab_type": "text"
      },
      "source": [
        "### Applying the grid search function for random forest only on the best features obtained using RFE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iiBoroV2Xr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search_random_forrest_best(X[['duration','euribor3m','age','nr.employed','job','day_of_week','campaign','education','emp.var.rate','poutcome']],y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAfCjYlw2bK-",
        "colab_type": "text"
      },
      "source": [
        "### Using Grid Search for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj5xDPrk2q_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid_search_log_reg(dataframe,target):\n",
        "    \n",
        "    \n",
        "    x_train,x_val,y_train,y_val = train_test_split(dataframe, target, test_size=0.3, random_state=42)\n",
        "\n",
        "    smote = SMOTE(kind='regular')\n",
        "    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n",
        "    \n",
        "    \n",
        "    log_reg = LogisticRegression()\n",
        "    \n",
        "    param_grid = { \n",
        "        'C' : np.logspace(-5, 8, 15)\n",
        "    }\n",
        "    grid_search = GridSearchCV(log_reg, param_grid=param_grid)\n",
        "    \n",
        "    grid_search.fit(X_sm, y_sm)\n",
        "    y_pred = grid_search.predict(x_val)\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_pred)\n",
        "    print('On Validation Data')\n",
        "    print('ROC_AUC_SCORE is ',roc_auc_score(y_val, y_pred))\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    plt.clf()\n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "grid_search_log_reg(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2JiXuk_2wiM",
        "colab_type": "text"
      },
      "source": [
        "### Applying XGBoost model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgi3SyY20rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xgboost(dataframe,target):\n",
        "    X = dataframe\n",
        "    y = target\n",
        "\n",
        "    x_train,x_val,y_train,y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    smote = SMOTE(kind='regular')\n",
        "    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n",
        "\n",
        "    model = XGBClassifier(n_estimators=50, max_depth=4)\n",
        "    model.fit(pd.DataFrame(X_sm,columns=x_train.columns), y_sm)\n",
        "    y_pred = model.predict(x_val)\n",
        "    \n",
        "    print('On Validation Data')\n",
        "    print('ROC_AUC_SCORE is ', roc_auc_score(y_val, y_pred))\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_pred)\n",
        "    #     print(auc(false_positive_rate, true_positive_rate))\n",
        "    \n",
        "    \n",
        "    plt.clf()\n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    print(confusion_matrix(y_val,y_pred))\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "    \n",
        "xgboost(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lzChjPs25Wa",
        "colab_type": "text"
      },
      "source": [
        "## Ensembling\n",
        "\n",
        "Ensemble learning uses multiple machine learning models  to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. In the below task, we have used an ensemble of three models - `RandomForestClassifier()`, `GradientBoostingClassifier()`, `LogisticRegression()`. Feel free to modify this function as per your requirements and fit more models or change the parameters for every model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcNjVLXu29pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "def ensemble_func2(dataframe,target):\n",
        "    \n",
        "    x_train,x_val,y_train,y_val = train_test_split(dataframe, target, test_size=0.3, random_state=42)\n",
        "\n",
        "    smote = SMOTE(kind='regular')\n",
        "    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n",
        "    \n",
        "    \n",
        "    model1 = RandomForestClassifier()\n",
        "    model3 = GradientBoostingClassifier()\n",
        "    model2 = LogisticRegression()\n",
        "    \n",
        "    model = VotingClassifier(estimators=[('rf', model1), ('lr', model2), ('xgb',model3)], voting='soft')\n",
        "    model.fit(X_sm,y_sm)\n",
        "    y_pred = model.predict(x_val)\n",
        "    model.score(x_val,y_val)\n",
        "\n",
        "        \n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_val, y_pred)\n",
        "    print('On Validation Data')\n",
        "    print('AUC_ROC_SCORE is ',roc_auc_score(y_val, y_pred))\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_val, y_pred))\n",
        "    print(confusion_matrix(y_val, y_pred))\n",
        "    \n",
        "    plt.clf()\n",
        "    plt.plot(false_positive_rate, true_positive_rate)\n",
        "    plt.xlabel('FPR')\n",
        "    plt.ylabel('TPR')\n",
        "    plt.title('ROC curve')\n",
        "    plt.show()\n",
        "    \n",
        "ensemble_func2(X,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHEZpXS3Cks",
        "colab_type": "text"
      },
      "source": [
        "## Prediction on the test data\n",
        "\n",
        "In the below task, we have performed a prediction on the test data. We have used Logistic Regression for this prediction. You can use the model of your choice that will give you the best metric score on the validation data. \n",
        "\n",
        "In this task below, we will read the test file and store the `Id` column from the test file in a variable `Id`. This column would be of use to us while submission since we need to have an Id column in the submission file which is the same Id of the observations in the test data.\n",
        "\n",
        "We have to perform the same preprocessing operations on the test data that we have performed on the train data. For demonstration purposes, we have preprocessed the test data and this preprocessed data is present in the csv file `test_preprocessed.csv`\n",
        "\n",
        "We then make a prediction on the preprocessed test data using the Grid Search Logisitic regression model. And as the final step, we concatenate this prediction with the `Id` column and then convert this into a csv file which becomes the `submission.csv` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BkmXBM3GW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actual Test File\n",
        "test = pd.read_csv('../data/test.csv')\n",
        "\n",
        "# Storing the Id column\n",
        "Id = test[['Id']]\n",
        "\n",
        "# Preprocessed Test File\n",
        "test = pd.read_csv('../data/test_preprocessed.csv')\n",
        "test.drop('Id',1,inplace=True)\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjsrHHF-3JGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid_search_log_reg(dataframe,target):\n",
        "\n",
        "\n",
        "    x_train,x_val,y_train,y_val = train_test_split(dataframe, target, test_size=0.3, random_state=42)\n",
        "\n",
        "    smote = SMOTE(kind='regular')\n",
        "    X_sm, y_sm =  smote.fit_sample(x_train, y_train)\n",
        "\n",
        "\n",
        "    log_reg = LogisticRegression()\n",
        "\n",
        "    param_grid = { \n",
        "        'C' : np.logspace(-5, 8, 15)\n",
        "    }\n",
        "    grid_search = GridSearchCV(log_reg, param_grid=param_grid)\n",
        "\n",
        "    grid_search.fit(X_sm, y_sm)\n",
        "    \n",
        "    # Predict on the preprocessed test file\n",
        "    y_pred = grid_search.predict(test)\n",
        "    return y_pred\n",
        "\n",
        "    \n",
        "prediction = pd.DataFrame(grid_search_log_reg(X,y),columns=['y'])\n",
        "submission = pd.concat([Id,prediction['y']],1)\n",
        "\n",
        "submission.to_csv('../data/submission.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}